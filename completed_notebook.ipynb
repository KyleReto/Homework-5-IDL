{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "me7MaEVG66Pb",
      "metadata": {
        "id": "me7MaEVG66Pb"
      },
      "source": [
        "# HW5: Unsupervised Speech Recognition (USR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CdBewqEe7Xjs",
      "metadata": {
        "id": "CdBewqEe7Xjs"
      },
      "source": [
        "Welcome to HW5 in Introduction to Deep Learning 11685. You will be working on Unsupervised Speech Recognition with GANs in this HW. You will be reimplementing and further improving on the model given in the USR paper by Facebook AI.<br>\n",
        "Link: https://arxiv.org/abs/2105.11084\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a4fd06f0",
      "metadata": {},
      "source": [
        "Most comments in the code below are from the given code, as I did not delete comments or TODOs as I filled them out."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vih9ZiCQ7skd",
      "metadata": {
        "id": "vih9ZiCQ7skd"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DgYdpT-l7v_C",
      "metadata": {
        "id": "DgYdpT-l7v_C"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/pytorch/fairseq\n",
        "! pip install torchsummaryX\n",
        "! pip install wandb -q\n",
        "# You can install other libraries such as torchsummaryX, wandb and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o52Nmb_976iy",
      "metadata": {
        "id": "o52Nmb_976iy"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "twxcI5cX78YN",
      "metadata": {
        "id": "twxcI5cX78YN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    # TODONE: Put your kaggle username & key here\n",
        "    # key deleted for obvious reasons\n",
        "    f.write('{\"username\":\"u\",\"key\":\"k\"}') \n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xvhE7WH48ZuK",
      "metadata": {
        "id": "xvhE7WH48ZuK"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c 11-685-s23-hw5 --force\n",
        "!mkdir '/content/data'\n",
        "\n",
        "!unzip -qo '/content/11-685-s23-hw5.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OmEqwUmr8bIR",
      "metadata": {
        "id": "OmEqwUmr8bIR"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2e63a8-73fe-44ba-952f-6a9b6d1a7150",
      "metadata": {
        "id": "dd2e63a8-73fe-44ba-952f-6a9b6d1a7150"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# add any other imports that you want \n",
        "\n",
        "has_cuda = torch.cuda.is_available()\n",
        "if has_cuda:\n",
        "  print(\"GPU: \", torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if has_cuda else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8526e858",
      "metadata": {},
      "source": [
        "Note that this cell pulls the contents of the .py files from Google Drive, as I ran this project on Colab and this way was easiest for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uki_6a_i8nDg",
      "metadata": {
        "id": "uki_6a_i8nDg"
      },
      "outputs": [],
      "source": [
        "# TODONE\n",
        "%cp -r \"/content/drive/MyDrive/School Stuff/Deep Learning/hw5_handout_S23\" \"/content/handout\"\n",
        "%cd /content/handout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x8t4UM6G802b",
      "metadata": {
        "id": "x8t4UM6G802b"
      },
      "source": [
        "# Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WahsDNk29slr",
      "metadata": {
        "id": "WahsDNk29slr"
      },
      "source": [
        "You have TODOs which need to be completed in `task/unpaired_audio_text.py` before you run these cells. You just need to replace the paths. You can use the original code base as a reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf0e76a-be23-4898-85fc-b8c20c0e9b7d",
      "metadata": {
        "id": "9cf0e76a-be23-4898-85fc-b8c20c0e9b7d"
      },
      "outputs": [],
      "source": [
        "from task import UnpairedAudioText\n",
        "\n",
        "task = UnpairedAudioText()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b5c5b4-f26f-4748-ae21-609a26b0f35a",
      "metadata": {
        "id": "48b5c5b4-f26f-4748-ae21-609a26b0f35a"
      },
      "outputs": [],
      "source": [
        "train_dataloader_args = dict(batch_size=160, #feel free to change these values\n",
        "                             num_workers=4,\n",
        "                            ) if has_cuda else dict(batch_size=64)\n",
        "train_dataloader_args[\"shuffle\"] = True\n",
        "train_dataloader_args[\"collate_fn\"] = task.datasets[\"train\"].collater\n",
        "\n",
        "validation_dataloader_args = train_dataloader_args.copy()\n",
        "validation_dataloader_args[\"shuffle\"] = False\n",
        "validation_dataloader_args[\"collate_fn\"] = task.datasets[\"valid\"].collater\n",
        "\n",
        "train_dataloader = data.DataLoader(task.datasets[\"train\"], **train_dataloader_args)\n",
        "validation_dataloader = data.DataLoader(task.datasets[\"valid\"], **validation_dataloader_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xxNbZ8Xpp_xN",
      "metadata": {
        "id": "xxNbZ8Xpp_xN"
      },
      "source": [
        "Code for Quiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OUJrGfVHmKy1",
      "metadata": {
        "id": "OUJrGfVHmKy1"
      },
      "outputs": [],
      "source": [
        "'''from fairseq.data import (\n",
        "    Dictionary,\n",
        "    data_utils,\n",
        "    StripTokenDataset,\n",
        ")\n",
        "import os'''\n",
        "'''text_dataset = data_utils.load_indexed_dataset(\n",
        "                os.path.join(task.cfg.text_data, 'train'), task.target_dictionary\n",
        "            )\n",
        "'''\n",
        "#print(f'The dataset has {len(task.datasets[\"train\"])} speech segments')\n",
        "#print(f'The dataset has {len(text_dataset)} text segments')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XVawLoVc-X8J",
      "metadata": {
        "id": "XVawLoVc-X8J"
      },
      "source": [
        "# Model and Training Configurations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "PrUXnHLM-fjp",
      "metadata": {
        "id": "PrUXnHLM-fjp"
      },
      "source": [
        "You need to complete the TODOs in `model/wav2vec_u.py` before you run this cell. You can use the original codebase as a refernce to complete this.\n",
        "Original Codebase: https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/unsupervised/.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11242dfc-d46b-4c96-b49a-474d4ce21415",
      "metadata": {
        "id": "11242dfc-d46b-4c96-b49a-474d4ce21415"
      },
      "outputs": [],
      "source": [
        "from model import Wav2vec_U\n",
        "\n",
        "model = Wav2vec_U(task.target_dictionary).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gvYhE31qF09H",
      "metadata": {
        "id": "gvYhE31qF09H"
      },
      "source": [
        "For a GAN, you need optimizers for both the discriminator and the generator. Configure the optimizers according to fairseq's configuration given in the link:\n",
        "https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/unsupervised/config/gan/w2vu.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O95VNc8FAJzh",
      "metadata": {
        "id": "O95VNc8FAJzh"
      },
      "outputs": [],
      "source": [
        "GENERATOR_CONFIG = {\n",
        "  \"lr\": 0.0004,\n",
        "  \"adam_betas\": (0.5,0.98),\n",
        "  \"adam_eps\": 1e-06,\n",
        "  \"weight_decay\": 0,\n",
        "}\n",
        "DISCRIMINATOR_CONFIG = {\n",
        "  \"lr\": 0.0003,\n",
        "  \"adam_betas\": (0.5,0.98),\n",
        "  \"adam_eps\": 1e-06,\n",
        "  \"weight_decay\": 0.0001,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4d1cf5-e9b8-46d6-a031-bc3c63607fc4",
      "metadata": {
        "id": "5c4d1cf5-e9b8-46d6-a031-bc3c63607fc4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "num_epochs = 2000\n",
        "epoch_start = 1\n",
        "\n",
        "if epoch_start == 1:\n",
        "    # define 2 optimizers for different parts of the model at the start of the training\n",
        "    optimizer = {\n",
        "      \"discriminator\": optim.Adam(model.discriminator.parameters(),\n",
        "                                  # TODONE: define lr, weight decay, betas and other relavant parameters\n",
        "                                  lr=DISCRIMINATOR_CONFIG[\"lr\"],\n",
        "                                  betas=DISCRIMINATOR_CONFIG[\"adam_betas\"],\n",
        "                                  eps=DISCRIMINATOR_CONFIG[\"adam_eps\"],\n",
        "                                  weight_decay=DISCRIMINATOR_CONFIG[\"weight_decay\"]\n",
        "\n",
        "                                 ),\n",
        "      \"generator\": optim.Adam(chain(model.generator.parameters(), model.segmenter.parameters()),\n",
        "                              # TODONE: define lr, weight decay, betas and other relavant parameters\n",
        "                              lr=GENERATOR_CONFIG[\"lr\"],\n",
        "                              betas=GENERATOR_CONFIG[\"adam_betas\"],\n",
        "                              eps=GENERATOR_CONFIG[\"adam_eps\"],\n",
        "                              weight_decay=GENERATOR_CONFIG[\"weight_decay\"]\n",
        "                              )\n",
        "    }\n",
        "    \n",
        "# Optional TODO: Consider using mixed-precision to speed up training\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6pJtsdZqFBBt",
      "metadata": {
        "id": "6pJtsdZqFBBt"
      },
      "source": [
        "A bunch of TODOs in the next cell. <br><br>\n",
        "Tip: Instead of completing whole `run_model` function and the debugging while running the experiment section, you can create a new cell and code your own sanity check. It may help you to understand what is returned from the dataloader, what needs to be pushed to the device, how model is called and what `loss_stats` are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AunCjF1SFYmr",
      "metadata": {
        "id": "AunCjF1SFYmr"
      },
      "outputs": [],
      "source": [
        "# Scheduler for use with the discriminator optimizer.\n",
        "# Freezes the LR for every other epoch, to give the generator time to catch up.\n",
        "def freeze_lr(epoch):\n",
        "  unfreeze_every = 2\n",
        "  if epoch % (unfreeze_every*2) == 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "# Uncommenting this line and one of the lines farther below will enable the freeze_lr scheduler\n",
        "#scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer['discriminator'], lr_lambda=freeze_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "899ce50f-c973-46b1-b9c2-6a49bfbf0fab",
      "metadata": {
        "id": "899ce50f-c973-46b1-b9c2-6a49bfbf0fab"
      },
      "outputs": [],
      "source": [
        "from numpy.core.multiarray import ndarray\n",
        "# Hint: You may find pdb to be a great tool in helping you understand returned values\n",
        "# from the dataloader and the model. Usage:\n",
        "# import pdb\n",
        "# pdb.set_trace()\n",
        "\n",
        "def run_model(model, dataloader):\n",
        "    cumulative_stats = dict()\n",
        "\n",
        "    for data in tqdm(dataloader, desc=\"Train\" if model.training else \"Eval \"):\n",
        "        net_input = data['net_input']\n",
        "        # What are the keys and values obtained from the data loader?\n",
        "        # TODONE: move all tensors to GPU\n",
        "        #data['id'] = data['id'].to(device)\n",
        "\n",
        "        data['net_input']['features'] = data['net_input']['features'].to(device)\n",
        "        data['net_input']['padding_mask'] = data['net_input']['padding_mask'].to(device)\n",
        "        # Tip: Checking what is inside net_input might help\n",
        "\n",
        "        if model.training:\n",
        "            # TODONE: We are training the model. Might need to do something with the optimizer?\n",
        "            data['net_input']['random_label'] = data['net_input']['random_label'].to(device)\n",
        "            if model.discrim_step(model.update_num):\n",
        "              optimizer['discriminator'].zero_grad()\n",
        "            else:\n",
        "              optimizer['generator'].zero_grad()\n",
        "            #model.zero_grad()\n",
        "            # Remember that you are training a GAN. Both optimizers won't be used at the same time.\n",
        "            # You may have to write an if statement or something similar to use the specific optimizer.\n",
        "            # You may have to use the discrim_step() attribute in the Wav2vec_U class\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "              loss_stats = model(**net_input) # forward pass\n",
        "            \n",
        "\n",
        "            total_loss = 0.0 \n",
        "\n",
        "            # TODONE: accumulate losses into total_loss for backprop during training\n",
        "            losses = loss_stats['losses']\n",
        "            for k, v in losses.items():\n",
        "              if v != None:\n",
        "                total_loss += v\n",
        "            # loss_stats[\"losses\"] is a dictionary containing various loss components\n",
        "            # some losses can be None if it's not used\n",
        "\n",
        "            total_loss /= net_input[\"features\"].size(0) # average by batch\n",
        "\n",
        "\n",
        "            group = model.get_groups_for_update() \n",
        "            # Look at what the get_groups_for_update() function does in the Wav2vec_U class\n",
        "              # Outputs either 'discriminator' or 'generator', depending on which is in use\n",
        "            # Can you try to think how you can use discrim_step() previously?\n",
        "              # discrim_step outputs a bool, true if it's stepping or false otherwise\n",
        "\n",
        "            # backprop loss and run the corresponding optimizer\n",
        "            scaler.scale(total_loss).backward() # This is a replacement for loss.backward()\n",
        "            scaler.step(optimizer[group]) # This is a replacement for optimizer.step()\n",
        "            scaler.update()\n",
        "\n",
        "\n",
        "        else:\n",
        "            # validation\n",
        "            loss_stats = task.valid_step(data, model)\n",
        "\n",
        "\n",
        "        # accumulate batch stats\n",
        "        for k, v in loss_stats.items():\n",
        "            if type(v) is dict:\n",
        "                # flatten inner dictionary\n",
        "                key_value_pairs = [(k + \"_\" + kn, vn) for kn, vn in v.items()]\n",
        "            else:\n",
        "                key_value_pairs = [(k, v)]\n",
        "\n",
        "            # accmulate all statistics into cumulative_stats, a dictionary\n",
        "            for pair in key_value_pairs:\n",
        "              key, value = pair\n",
        "              if value == None:\n",
        "                continue\n",
        "              if torch.is_tensor(value):\n",
        "                value = value.cpu().detach().numpy()\n",
        "              if key in cumulative_stats:\n",
        "                cumulative_stats[key] += value\n",
        "              else:\n",
        "                cumulative_stats[key] = value\n",
        "            # NOTE: you should convert any returned tensors to either values or numpy arrays\n",
        "            # cumulative_stats shouldn't have nested dictionaries\n",
        "\n",
        "        del net_input, loss_stats\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # average stats over the dataset\n",
        "    # Note that some metrics are already averaged over batch, so the result won't make sense\n",
        "    # You can fix them if needed\n",
        "    for k, v in cumulative_stats.items():\n",
        "        v = v / len(dataloader.dataset)\n",
        "        if type(v) is np.ndarray:\n",
        "            v = v.tolist()\n",
        "        cumulative_stats[k] = v\n",
        "\n",
        "    return cumulative_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-P7cxaSNGi8w",
      "metadata": {
        "id": "-P7cxaSNGi8w"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qZehk9Axs3dG",
      "metadata": {
        "id": "qZehk9Axs3dG"
      },
      "outputs": [],
      "source": [
        "# just now noticed the unused parameter here, but I'm not changing it in case it breaks something else.\n",
        "def save_model(model, optimizers, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'discriminator_optimizer_state_dict'     : optimizer['discriminator'].state_dict(),\n",
        "         'generator_optimizer_state_dict'     : optimizer['generator'].state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "  \n",
        "def load_model(path, model, metric= 'edit_distance', optimizers= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizers != None:\n",
        "        optimizers['discriminator'].load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
        "        optimizers['generator'].load_state_dict(checkpoint['generator_optimizer_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    #metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, epoch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XPS77-phtlDB",
      "metadata": {
        "id": "XPS77-phtlDB"
      },
      "outputs": [],
      "source": [
        "best_edit_dist = 37 # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = '/content/drive/MyDrive/pth/HW5_temp.pth'# set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = '/content/drive/MyDrive/pth/HW5_final.pth'# set best model path "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kakGXiLOrW6x",
      "metadata": {
        "id": "kakGXiLOrW6x"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"50b736c0fcb136ffc188e014a038e939c5a4f3f4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QCm3PVPKOdyZ",
      "metadata": {
        "id": "QCm3PVPKOdyZ"
      },
      "outputs": [],
      "source": [
        "model, optimizer, epoch = load_model(best_model_path, model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jpDlIcCyraCT",
      "metadata": {
        "id": "jpDlIcCyraCT"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"Reduced LR + Augmentations Finetune\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = '9qelwp3k',### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw5-ablations\", ### Project should be created in your wandb account \n",
        "    # This uses only the generator config because it was a quick hacky fix that I didn't feel the need to rewrite later.\n",
        "    config = GENERATOR_CONFIG ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a4fe6c-a9e4-4c6e-9483-4fc4e1f5d3f9",
      "metadata": {
        "id": "09a4fe6c-a9e4-4c6e-9483-4fc4e1f5d3f9"
      },
      "outputs": [],
      "source": [
        "print(f\"Training for {num_epochs} epochs\", file=sys.stderr)\n",
        "\n",
        "eval_interval = 10 # evaluation after how many epochs?\n",
        "\n",
        "for epoch in range(epoch_start, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}\", file=sys.stderr)\n",
        "\n",
        "    model.train()\n",
        "    # The model uses the epoch number to decide which part of the network to train\n",
        "    model.set_num_updates(epoch) # Look at what this function does in the Wav2vec_U class\n",
        "\n",
        "    train_stats = run_model(model, train_dataloader)\n",
        "\n",
        "    print(train_stats)\n",
        "    if epoch % eval_interval == 0:\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          eval_stats = run_model(model, validation_dataloader)\n",
        "          \n",
        "      # TODONE: perhaps save your model and optimizer here\n",
        "      dist = eval_stats['edit_distance']\n",
        "      save_model(model, optimizer, ['edit_distance', dist], epoch, epoch_model_path)\n",
        "      print(\"Saved epoch model\")\n",
        "\n",
        "      if dist <= best_edit_dist:\n",
        "          best_edit_dist = dist\n",
        "          save_model(model, optimizer, ['edit_distance', dist], epoch, best_model_path)\n",
        "          print(\"Saved best model\")\n",
        "        # You may find it interesting to explore Wandb Artifcats to version your models\n",
        "          # Tip: You can even save the model after every epoch along with the best model. \n",
        "          # This may help to continue training even if the best model is from a very early epoch.\n",
        "\n",
        "      # TODONE: Log training/eval statistics\n",
        "      try:\n",
        "        wandb.log(train_stats)\n",
        "        wandb.log(eval_stats)\n",
        "      except:\n",
        "        print('Wand logging failed')\n",
        "      #scheduler.step()\n",
        "      print(eval_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa463D7CGwED",
      "metadata": {
        "id": "fa463D7CGwED"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d60ff70-589d-49b4-9445-0587cf538070",
      "metadata": {
        "id": "4d60ff70-589d-49b4-9445-0587cf538070"
      },
      "outputs": [],
      "source": [
        "from dataset import extracted_features_dataset\n",
        "# TODONE: PathToTest\n",
        "test_path = \"/content/data/11-685-s23-hw5/test\"\n",
        "testset = extracted_features_dataset.ExtractedFeaturesDataset(path=test_path,\n",
        "                                                              split='test')\n",
        "\n",
        "test_dataloader_args = train_dataloader_args.copy()\n",
        "test_dataloader_args[\"shuffle\"] = False\n",
        "test_dataloader_args[\"collate_fn\"] = testset.collater\n",
        "\n",
        "test_dataloader = data.DataLoader(testset, **test_dataloader_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t87M7kvdIGgI",
      "metadata": {
        "id": "t87M7kvdIGgI"
      },
      "source": [
        "Write the `test_step` function which can be coded very similar to `valid_step` given in `task/unpaired_audio_text.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WdfayF0SH9QM",
      "metadata": {
        "id": "WdfayF0SH9QM"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import editdistance\n",
        "\n",
        "# Based on the given code\n",
        "def test_step(inputs, model):\n",
        "    res = model(\n",
        "        **inputs[\"net_input\"],\n",
        "        dense_x_only=True,\n",
        "    )\n",
        "\n",
        "    dense_x = res[\"logits\"]\n",
        "    padding_mask = res[\"padding_mask\"]\n",
        "\n",
        "    z = dense_x.argmax(-1)\n",
        "    z[padding_mask] = task.target_dictionary.pad()\n",
        "\n",
        "    output = []\n",
        "    for i, (x, t, id) in enumerate(\n",
        "        zip(\n",
        "            z,\n",
        "            inputs[\"target\"] if \"target\" in inputs else [None] * len(z),\n",
        "            inputs[\"id\"],\n",
        "        )\n",
        "    ):\n",
        "\n",
        "        pred_units_arr = x\n",
        "\n",
        "        pred_units_arr = pred_units_arr.tolist()\n",
        "\n",
        "        chars = []\n",
        "        for char in pred_units_arr:\n",
        "            chars.append(task.target_dictionary.string([char]))\n",
        "        output.append(chars)\n",
        "        #output.append(pred_units_arr)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uo6nr_8tIkI4",
      "metadata": {
        "id": "uo6nr_8tIkI4"
      },
      "source": [
        "Write some code to evaluate and get the results. You are free to write the below cells however you want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D_F9axpLIh2P",
      "metadata": {
        "id": "D_F9axpLIh2P"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "results = []\n",
        "model, optimizer, epoch = load_model(best_model_path, model, optimizer)\n",
        "\n",
        "for data in tqdm(test_dataloader, desc=\"Test\"):\n",
        "    data['net_input']['features'] = data['net_input']['features'].to(device)\n",
        "    data['net_input']['padding_mask'] = data['net_input']['padding_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        out = test_step(data, model)\n",
        "        for batch in out:\n",
        "          results.append(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lT2TutVbJOYh",
      "metadata": {
        "id": "lT2TutVbJOYh"
      },
      "outputs": [],
      "source": [
        "# TODONE: Replace the path and get the phoneme_map.json for mapping\n",
        "with open(\"/content/data/11-685-s23-hw5/test/phoneme_map.json\", \"r\") as file:\n",
        "    phon_map = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y6w0wPUiJcCO",
      "metadata": {
        "id": "Y6w0wPUiJcCO"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for line in results:\n",
        "    prediction = \"\".join([phon_map[index] for index in line])\n",
        "    # TODONE: Map results with phon_map\n",
        "    predictions.append(prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDnijOksJqMj",
      "metadata": {
        "id": "bDnijOksJqMj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# TODONE: Make the CSV and submit to kaggle\n",
        "data_dir = '/content/data/11-685-s23-hw5/test/sample_submission.csv'\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = predictions\n",
        "df.to_csv('submission.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "me7MaEVG66Pb",
        "vih9ZiCQ7skd",
        "o52Nmb_976iy",
        "OmEqwUmr8bIR"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
